{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StrategicalIT/PipedPiperAI/blob/main/Lab09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cae130-b662-46d8-9b14-da63c83f8b31",
      "metadata": {
        "id": "a6cae130-b662-46d8-9b14-da63c83f8b31"
      },
      "source": [
        "# LAB 9: Full RAG solution with LlamaIndex\n",
        "In this lab we are going to build a full RAG solution using the LlamaIndex framework that leverages models from the Nvidia NIM API catalog (including LLM and embedding models). For this, you will need an API key from NVIDIA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fbd44bf-1c1c-4591-aeb2-7e71b3f40e65",
      "metadata": {
        "id": "7fbd44bf-1c1c-4591-aeb2-7e71b3f40e65"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9189748d-cb87-4066-83ed-8843d49e7118",
      "metadata": {
        "id": "9189748d-cb87-4066-83ed-8843d49e7118"
      },
      "source": [
        "The first step is to install the necessary libraries. This installs the core llama-index package which draws a lot of dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a",
      "metadata": {
        "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e15b7a-6bd7-4e72-b7dd-e67896a57aca",
      "metadata": {
        "id": "62e15b7a-6bd7-4e72-b7dd-e67896a57aca"
      },
      "source": [
        "If you look carefully at the previous output you will notice that the only llm interface that has been installed is OpenAI. We are going to use NVIDIA NIMs including LLMs and Embedding models, so we need to install their corresponding modules.\n",
        "\n",
        "You can see what LLM modules are available in LlamaIndex in [https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79ddf20-3824-4419-88ec-9e3b8c8ae63a",
      "metadata": {
        "id": "e79ddf20-3824-4419-88ec-9e3b8c8ae63a"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-nvidia llama-index-embeddings-nvidia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a89838-296d-4b97-a91f-97552c4a5d0b",
      "metadata": {
        "id": "14a89838-296d-4b97-a91f-97552c4a5d0b"
      },
      "source": [
        "Now we can import the components we need for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae207cd-f93f-48c5-90ef-ab838e012fe7",
      "metadata": {
        "id": "6ae207cd-f93f-48c5-90ef-ab838e012fe7"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.nvidia import NVIDIA\n",
        "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
        "import urllib3\n",
        "urllib3.disable_warnings()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34e6f10-da13-4d11-9372-c022bc5fd2d1",
      "metadata": {
        "id": "e34e6f10-da13-4d11-9372-c022bc5fd2d1"
      },
      "source": [
        "## Instantiate the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to setup the NVIDIA API key."
      ],
      "metadata": {
        "id": "hq8BAXIHi-SY"
      },
      "id": "hq8BAXIHi-SY"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#apikey = os.environ[\"NVIDIA_API_KEY\"]\n",
        "#change from OS variable import to using Google Colab secret\n",
        "from google.colab import userdata\n",
        "apikey = userdata.get('apikey')\n",
        "os.environ[\"NVIDIA_API_KEY\"] = apikey\n",
        "#print(apikey)"
      ],
      "metadata": {
        "id": "wrhE3VMijDyn"
      },
      "id": "wrhE3VMijDyn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325",
      "metadata": {
        "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325"
      },
      "source": [
        "LlamaIndex provides a \"Settings\" object that stores the most commonly used resources in a LlamaIndex workflow, ex: llm, embed_model. It is a sort of a global storage place for all default settings. If one attribute is not provided anywhere else in the code, the \"Settings\" object will be queried. As you will see there are several default values that are assumed if not specified. This makes the code look cleaner.\n",
        "\n",
        "The following line will be sufficient to instantiate an LLM from the Nvidia NIM API if the right defaults apply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46582ab-3ecc-433f-8533-e4aa557ed1b9",
      "metadata": {
        "id": "b46582ab-3ecc-433f-8533-e4aa557ed1b9"
      },
      "outputs": [],
      "source": [
        "Settings.llm = NVIDIA()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c5a06e-524e-4674-928d-647efa28361c",
      "metadata": {
        "id": "41c5a06e-524e-4674-928d-647efa28361c"
      },
      "source": [
        "When the parameter ```base_url``` is not explicitly defined and we are using ```NVIDIA()``` then it assumes ```base_url = \"https://integrate.api.nvidia.com/v1\"```\n",
        "\n",
        "If the parameter ```api_key``` is not omitted then it will try to read the variable ```NVIDIA_API_KEY``` from the environment\n",
        "\n",
        "Also, like in previous NIM examples, if ```model``` is not present then it will assume ```meta/llama3-8b-instruct```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8acc0cb1-6eec-444f-b6a7-81ead015727f",
      "metadata": {
        "id": "8acc0cb1-6eec-444f-b6a7-81ead015727f"
      },
      "source": [
        "So for example, let's say you want to connect to a NIM that is running locally in your datacenter, you want to hard-code the key explicitly in your code and you want to use a Mistral model. Then, the Settings would look like this\n",
        "```\n",
        "Settings.llm = NVIDIA(\n",
        "    base_url=\"http://nim-host-address:8000/v1\",\n",
        "    api_key = \"nvapi-123456789abcdefg\",\n",
        "    model=\"mistralai/mistral-7b-instruct-v0.2\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca8be7e-b368-4a8f-a046-52a9af901202",
      "metadata": {
        "id": "8ca8be7e-b368-4a8f-a046-52a9af901202"
      },
      "source": [
        "We can verify what model we are pointing to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628b9eb6-ec8d-498e-bc64-acb723dffdca",
      "metadata": {
        "id": "628b9eb6-ec8d-498e-bc64-acb723dffdca"
      },
      "outputs": [],
      "source": [
        "print(\"... Using: \", Settings.llm.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee8bce9-e03f-494d-832c-a9827084d0bd",
      "metadata": {
        "id": "bee8bce9-e03f-494d-832c-a9827084d0bd"
      },
      "source": [
        "## Load the documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e729d76-c448-46c5-9284-c4845a49116d",
      "metadata": {
        "id": "6e729d76-c448-46c5-9284-c4845a49116d"
      },
      "source": [
        "Let's use \"Simple Directory Reader\" to load the documents in the \"data\" directory. The data directory could also be a mounted directory pointing to PowerScale.\n",
        "\n",
        "SimpleDirectoryReader will ingest Markdown, PDFs, Word documents, PowerPoint decks, images, audio and video from the specified directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4260f76-1fb5-4e53-ac65-5f27294c34e1",
      "metadata": {
        "id": "f4260f76-1fb5-4e53-ac65-5f27294c34e1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "folder_name = \"PipedPiperAIData\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "!wget -O PipedPiperAIData/r760xa.pdf https://www.delltechnologies.com/asset/en-us/products/servers/technical-support/poweredge-r760xa-spec-sheet.pdf\n",
        "print(f\"Folder '{folder_name}' created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now upload documents into the folder created in the above step. You do this via the Colab UI by clicking on the folder icon (on the left side menu), then uploading docs.\n",
        "\n",
        "Wait for the documents to **fully upload** before running the next code cell!"
      ],
      "metadata": {
        "id": "IiIcP8UNk7a5"
      },
      "id": "IiIcP8UNk7a5"
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = os.path.join(\"/content/\", folder_name)\n",
        "print(folder_name)\n",
        "documents = SimpleDirectoryReader(folder_name).load_data()\n",
        "#print(documents)"
      ],
      "metadata": {
        "id": "6qy9gpHhlcAE"
      },
      "id": "6qy9gpHhlcAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a44c997a-73ed-48fd-aaf8-404ae93b778c",
      "metadata": {
        "id": "a44c997a-73ed-48fd-aaf8-404ae93b778c"
      },
      "source": [
        "## Instantiate the embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43eb6d62-2a79-4c55-8c55-9388f1574756",
      "metadata": {
        "id": "43eb6d62-2a79-4c55-8c55-9388f1574756"
      },
      "source": [
        "We are going to use the \"Settings\" object again but this time for the embedding model. Instead of requesting a specific embed model, we leave ```model``` blank and it will select the default one for the NVIDIA module. Also, since we didn't specify ```api_key``` it will attempt to find ```NVIDIA_API_KEY``` in the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33b3df4-ddf8-45b2-8091-3a1f11e2b871",
      "metadata": {
        "id": "e33b3df4-ddf8-45b2-8091-3a1f11e2b871"
      },
      "outputs": [],
      "source": [
        "Settings.embed_model = NVIDIAEmbedding(truncate=\"END\")\n",
        "#print(\"... Using embedding model: \", Settings.embed_model.model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc80c276-a98d-489b-a8ec-48a099e482da",
      "metadata": {
        "id": "bc80c276-a98d-489b-a8ec-48a099e482da"
      },
      "source": [
        "## Create an Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "525e2aa7-ac11-4bc2-b9aa-0d78200639b0",
      "metadata": {
        "id": "525e2aa7-ac11-4bc2-b9aa-0d78200639b0"
      },
      "source": [
        "Index is a key contruct in the LlamaIndex framework. To build it we need \"Documents\", an \"Embedding model\" and a \"Storage Context\". \"Storage Context\" is optional because when not specified, LlamaIndex uses a simple in-memory vector store that's great for quick experimentation.\n",
        "\n",
        "Notice also how we are not specifying the ```embed_model``` parameter because it is already defined in the ```Settings``` object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a2cd4a-8e94-41b6-9a48-3f4faef53be3",
      "metadata": {
        "id": "a1a2cd4a-8e94-41b6-9a48-3f4faef53be3"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n",
        "print(f\"Number of chunks in the index: {len(index.docstore.docs)}\")\n",
        "# Assuming 'folder_name' still holds the path to your directory\n",
        "file_count = len([f for f in os.listdir(folder_name) if os.path.isfile(os.path.join(folder_name, f))])\n",
        "print(f\"Number of physical files in the folder: {file_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8a69fd-f1aa-42b5-98fe-59e3c754d86d",
      "metadata": {
        "id": "1f8a69fd-f1aa-42b5-98fe-59e3c754d86d"
      },
      "source": [
        "## Build the query engine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda25e38-733a-4f07-bb11-64e399f36179",
      "metadata": {
        "id": "cda25e38-733a-4f07-bb11-64e399f36179"
      },
      "source": [
        "The final step is to use the \"LLM\" and the \"Index\" to build the \"Query Engine\". Thanks again to the ```Setttings``` object we don't need to specify ```llm=llm``` or ```embed_model=embed_model```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c3b3f5-4666-47a3-b050-3d55552ceae6",
      "metadata": {
        "id": "d7c3b3f5-4666-47a3-b050-3d55552ceae6"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "print(f\"Type of query_engine: {type(query_engine)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d514f72-378c-4f0b-b364-e04b1d6ad275",
      "metadata": {
        "id": "6d514f72-378c-4f0b-b364-e04b1d6ad275"
      },
      "source": [
        "## Query the RAG solution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134deb68-bf09-454b-8863-3d9f056cd5af",
      "metadata": {
        "id": "134deb68-bf09-454b-8863-3d9f056cd5af"
      },
      "source": [
        "Everything is ready to start querying our RAG solution. We use the ```.query()``` method from the ```query_engine```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7370131d-fe54-4d4c-81bd-5db819988447",
      "metadata": {
        "id": "7370131d-fe54-4d4c-81bd-5db819988447"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"How many pcie slots are there in a Dell R760xa?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5359022f-c09d-4634-90d9-a9fc39e07c39",
      "metadata": {
        "id": "5359022f-c09d-4634-90d9-a9fc39e07c39"
      },
      "source": [
        "## End of Lab 9"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}